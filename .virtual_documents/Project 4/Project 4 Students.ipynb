








# Import libraries
import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
%matplotlib inline
import seaborn as sns
from sklearn.model_selection import train_test_split
sns.set_style("darkgrid")


# Load nhanes data
nhanes = pd.read_csv('nhanes.csv')
# Get the ID numbers for each observation (seqn)
seqn = nhanes['SEQN']
# Get the target, "self-reported health condition," HSD010
hsd010 = nhanes['HSD010']
# Drop SEQN from the dataframe and then apply the standard scaler
nhanes = nhanes.drop(['SEQN', 'HSD010'], axis = 1)
nhanes_scaled = pd.DataFrame(StandardScaler().fit_transform(nhanes),
                             columns = nhanes.columns)
# Add the ID and target back in
nhanes_scaled['SEQN'] = seqn
nhanes_scaled['HSD010'] = hsd010
nhanes_scaled = nhanes_scaled.set_index('SEQN')
nhanes_scaled.head()








# Create a binary version of hsd010 where 1-3 are "good" and 4-5 are "poor"
nhanes_scaled['HSD010_binary'] = hsd010_binary = nhanes_scaled['HSD010'].replace(
    [1, 2, 3, 4, 5], ['good', 'good', 'good', 'poor', 'poor']) 
# Recode the original hsd010 with the string labels
nhanes_scaled['HSD010'] = nhanes_scaled['HSD010'].replace(
    [1, 2, 3, 4, 5], ['excellent', 'very good', 'good', 'fair', 'poor'])
# Boxplot of hsd010
ax = sns.boxplot(x = 'HSD010', y = 'INDFMPIR', data = nhanes_scaled)
ax.set(xlabel = "Self Reported Health Condition",
      ylabel = "Family Income to Poverty Line Ratio")
ax.set_title("Boxplot of Family Income to Poverty Line Ratio by Self-Reported Health Status")
plt.show()


# Boxplot of hsd010_binary
ax = sns.boxplot(x = 'HSD010_binary', y = 'INDFMPIR', data = nhanes_scaled)
ax.set(xlabel = "Self Reported Health Condition",
      ylabel = "Family Income to Poverty Line Ratio")
ax.set_title("Boxplot of Family Income to Poverty Line Ratio by Binary Self-Reported Health Status")
plt.show()





ax = sns.scatterplot(x = "INDFMPIR", y = "BMXBMI", hue = "HSD010", palette = "tab10", data = nhanes_scaled)
ax.set(xlabel = "Income to Poverty Line Ratio",
      ylabel = "Body Mass Index")
ax.set_title("BMI v. Income-Poverty Ratio")
plt.show()





nhanes_scaled = nhanes_scaled.drop(['HSD010', 'HSD010_binary'], axis = 1)











pca_full = PCA()
pca_full.fit(nhanes_scaled)

# Method 1
variance = np.cumsum(pca_full.explained_variance_ratio_)
n_90 = np.where(variance >= 0.90)[0][0] + 1
print(n_90)
# 120 components explain 90 percent of the variation 

n_75 = np.where(variance >= 0.75)[0][0] + 1
print(n_75)
# 73 components explain 75 percent of the variation 

n_50 = np.where(variance >= 0.50)[0][0] + 1
print(n_50)
# 25 components explain 50 percent of the variation 

# Method 2
explained_variance = pca_full.explained_variance_ratio_

fig, ax = plt.subplots()
ax.bar(range(1, 121), explained_variance[:120])
ax.set(xlabel='Component',
       ylabel='Variance Explained')
ax.set_title('Variance Explained by Each Component')
plt.show()








fig, ax = plt.subplots()
ax.bar(range(1, 242), explained_variance[:242])
ax.set(xlabel='Component',
       ylabel='Variance Explained')
ax.set_title('Variance Explained by Each Component (All 242 Components)')
plt.show()

fig, ax = plt.subplots()
ax.bar(range(1, 121), explained_variance[:120])
ax.set(xlabel='Component',
       ylabel='Variance Explained')
ax.set_title('Variance Explained by Each Component (Top 120 Components)')
plt.show()

fig, ax = plt.subplots()
ax.bar(range(1, 21), explained_variance[:20])
ax.set(xlabel='Component',
       ylabel='Variance Explained')
ax.set_title('Variance Explained by Each Component (Top 20 Components)')
plt.show()





n_components_model = 15
pca_model = PCA(n_components=n_components_model)
X_pca_model = pca_model.fit_transform(nhanes_scaled)

print(n_components_model)
print(pca_model.explained_variance_ratio_.sum())








pca_df = pd.DataFrame(data=X_pca_model[:, :2], 
                      columns=['PC1', 'PC2'])

pca_df['HSD010'] = hsd010.map({1: 'excellent', 2: 'very good', 3: 'good', 4: 'fair', 5: 'poor'})

fig, ax = plt.subplots()
sns.scatterplot(x='PC1', 
                y='PC2', 
                hue='HSD010', 
                hue_order=['excellent', 'very good', 'good', 'fair', 'poor'],
                palette='tab10', 
                data=pca_df)
ax.set(xlabel=f'PC1 ({pca_model.explained_variance_ratio_[0]:.2%} variance)',
       ylabel=f'PC2 ({pca_model.explained_variance_ratio_[1]:.2%} variance)')
ax.set_title('Scatterplot of First Two Components')
plt.show()























inertias = []
k_range = range(2, 11)

for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(nhanes_scaled)
    inertias.append(kmeans.inertia_)

# Plot the elbow curve
plt.figure()
plt.plot(k_range, inertias, 'bo-')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Inertia')
plt.title('Elbow Method for Optimal k')
plt.xticks(k_range)
plt.show()

kmeans_original = KMeans(n_clusters=4, random_state=42, n_init=10)
cluster_labels_original = kmeans_original.fit_predict(nhanes_scaled)
print(pd.Series(cluster_labels_original).value_counts().sort_index())








plot_df = pd.DataFrame({
    'INDFMPIR': nhanes_scaled['INDFMPIR'],
    'BMXBMI': nhanes_scaled['BMXBMI'],
    'Cluster': cluster_labels_original
})

fig, ax = plt.subplots(figsize=(12, 7))
sns.scatterplot(
    x='INDFMPIR',
    y='BMXBMI',
    hue='Cluster',
    palette='tab10',
    data=plot_df,
    ax=ax
)
ax.set(xlabel='Income to Poverty Line Ratio',
       ylabel='Body Mass Index (BMI)')
ax.set_title('K-Means Clusters: BMI vs Income-Poverty Ratio')
plt.show()








kmeans_pca = KMeans(n_clusters=4, random_state=42, n_init=10)
cluster_labels_pca = kmeans_pca.fit_predict(pca_df[['PC1', 'PC2']])  # Only use numeric columns

print(pd.Series(cluster_labels_pca).value_counts().sort_index())

plot_df_pca = pca_df[['PC1', 'PC2']].copy()
plot_df_pca['Cluster'] = cluster_labels_pca

fig, ax = plt.subplots(figsize=(12, 7))
sns.scatterplot(
    x='PC1',
    y='PC2',
    hue='Cluster',
    palette='tab10',
    data=plot_df_pca,
    ax=ax
)
ax.set(xlabel=f'PC1 ({pca_model.explained_variance_ratio_[0]:.2%} variance)',
       ylabel=f'PC2 ({pca_model.explained_variance_ratio_[1]:.2%} variance)')
ax.set_title('K-Means Clusters on PCA Space')
plt.show()














# partition data
# -----------
y = hsd010.values # either hsd010 or hsd010_binary, may need to convert to numeric if it isn't already 
X = nhanes_scaled # drop out any columns that aren't features

X_train, X_test, y_train, y_test = train_test_split(X, 
                                                    y, 
                                                    test_size = .25, 
                                                    random_state = 10)


# load libraries
# -----------
import keras
from keras.utils import to_categorical

num_classes = ...
# convert class vectors to binary class matrices
y_train = keras.utils.to_categorical(..., ...)
y_test = keras.utils.to_categorical(..., ...)
num_classes = y_test.shape[1]


# create neural network model
# -----------
model = Sequential()

model.add(Dense(..., input_dim= ..., kernel_initializer= ..., activation= ...))

model.add(Dense(..., kernel_initializer= ..., activation= ...))

## Add any additional layers you wish here

model.compile(loss= ..., optimizer= ..., metrics=[...])

model.fit(..., ..., validation_data=(..., ...), epochs=..., batch_size=..., verbose=...)





## Your Answer Here





## Your Answer Here












